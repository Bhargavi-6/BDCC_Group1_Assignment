{"cells":[{"cell_type":"markdown","source":["## IPL Data Description and Ingestion"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c5a5c279-5231-44b4-8de8-fc63f69f49bf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9816a491-e87e-4413-8ed6-1c1aafce307a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=2011388749505057#setting/sparkui/0306-093934-o5gb50jn/driver-8563294957346676801\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=2011388749505057#setting/sparkui/0306-093934-o5gb50jn/driver-8563294957346676801\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"markdown","source":["## Loading the dataset\n\nData is provided in *csv* format. Spark provides *sqlContext.read.csv* to read the csv data and create the dataframe. \nSpark can automatically infer the schema from the data if *inferSchema* is set to True."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4ab34f76-dce7-4178-a34b-92e21047dd76","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%fs ls /FileStore/tables/ipl_datasets/"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e9476ad6-985e-4fff-81d9-afd53ac4f0e1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/FileStore/tables/ipl_datasets/deliveries.csv","deliveries.csv",15442270,1678101160000],["dbfs:/FileStore/tables/ipl_datasets/matches.csv","matches.csv",117096,1678101115000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/ipl_datasets/deliveries.csv</td><td>deliveries.csv</td><td>15442270</td><td>1678101160000</td></tr><tr><td>dbfs:/FileStore/tables/ipl_datasets/matches.csv</td><td>matches.csv</td><td>117096</td><td>1678101115000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Read records using custom schema"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f3a56475-1362-49e2-b865-4a87032ec42d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\n# Define schema for matches.csv\nmatches_schema = StructType([\n    StructField(\"id\", IntegerType(), True),\n    StructField(\"season\", IntegerType(), True),\n    StructField(\"city\", StringType(), True),\n    StructField(\"date\", StringType(), True),\n    StructField(\"team1\", StringType(), True),\n    StructField(\"team2\", StringType(), True),\n    StructField(\"toss_winner\", StringType(), True),\n    StructField(\"toss_decision\", StringType(), True),\n    StructField(\"result\", StringType(), True),\n    StructField(\"dl_applied\", IntegerType(), True),\n    StructField(\"winner\", StringType(), True),\n    StructField(\"win_by_runs\", IntegerType(), True),\n    StructField(\"win_by_wickets\", IntegerType(), True),\n    StructField(\"player_of_match\", StringType(), True),\n    StructField(\"venue\", StringType(), True),\n    StructField(\"umpire1\", StringType(), True),\n    StructField(\"umpire2\", StringType(), True),\n    StructField(\"umpire3\", StringType(), True)\n])\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5b9a1ddc-8349-4199-929c-7ca272613fcd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["matches_df = sqlContext.read.csv(\"dbfs:/FileStore/tables/ipl_datasets/matches.csv\", \n                                header = True, \n                                schema = StructType(matches_schema) )\nmatches_df.cache()\nmatches_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b4ac9631-633e-464f-a3aa-219a948226ac","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- id: integer (nullable = true)\n |-- season: integer (nullable = true)\n |-- city: string (nullable = true)\n |-- date: string (nullable = true)\n |-- team1: string (nullable = true)\n |-- team2: string (nullable = true)\n |-- toss_winner: string (nullable = true)\n |-- toss_decision: string (nullable = true)\n |-- result: string (nullable = true)\n |-- dl_applied: integer (nullable = true)\n |-- winner: string (nullable = true)\n |-- win_by_runs: integer (nullable = true)\n |-- win_by_wickets: integer (nullable = true)\n |-- player_of_match: string (nullable = true)\n |-- venue: string (nullable = true)\n |-- umpire1: string (nullable = true)\n |-- umpire2: string (nullable = true)\n |-- umpire3: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["matches_df.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a3160af2-c7f6-48f0-bf55-1ef114cccbc7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------+---------+----------+--------------------+--------------------+--------------------+-------------+------+----------+--------------------+-----------+--------------+---------------+--------------------+--------------+-------------+-------+\n| id|season|     city|      date|               team1|               team2|         toss_winner|toss_decision|result|dl_applied|              winner|win_by_runs|win_by_wickets|player_of_match|               venue|       umpire1|      umpire2|umpire3|\n+---+------+---------+----------+--------------------+--------------------+--------------------+-------------+------+----------+--------------------+-----------+--------------+---------------+--------------------+--------------+-------------+-------+\n|  1|  2017|Hyderabad|2017-04-05| Sunrisers Hyderabad|Royal Challengers...|Royal Challengers...|        field|normal|         0| Sunrisers Hyderabad|         35|             0|   Yuvraj Singh|Rajiv Gandhi Inte...|   AY Dandekar|     NJ Llong|   null|\n|  2|  2017|     Pune|2017-04-06|      Mumbai Indians|Rising Pune Super...|Rising Pune Super...|        field|normal|         0|Rising Pune Super...|          0|             7|      SPD Smith|Maharashtra Crick...|A Nand Kishore|       S Ravi|   null|\n|  3|  2017|   Rajkot|2017-04-07|       Gujarat Lions|Kolkata Knight Ri...|Kolkata Knight Ri...|        field|normal|         0|Kolkata Knight Ri...|          0|            10|        CA Lynn|Saurashtra Cricke...|   Nitin Menon|    CK Nandan|   null|\n|  4|  2017|   Indore|2017-04-08|Rising Pune Super...|     Kings XI Punjab|     Kings XI Punjab|        field|normal|         0|     Kings XI Punjab|          0|             6|     GJ Maxwell|Holkar Cricket St...|  AK Chaudhary|C Shamshuddin|   null|\n|  5|  2017|Bangalore|2017-04-08|Royal Challengers...|    Delhi Daredevils|Royal Challengers...|          bat|normal|         0|Royal Challengers...|         15|             0|      KM Jadhav|M Chinnaswamy Sta...|          null|         null|   null|\n+---+------+---------+----------+--------------------+--------------------+--------------------+-------------+------+----------+--------------------+-----------+--------------+---------------+--------------------+--------------+-------------+-------+\nonly showing top 5 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### Memory Consumed to store the dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"10e91f20-4ec0-4f87-8f3d-39766ee4b8d0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark._jvm.org.apache.spark.util.SizeEstimator.estimate(matches_df._jdf)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cb42e067-4324-48b8-b8ba-f27ae558e7f2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[10]: 149376080"]}],"execution_count":0},{"cell_type":"markdown","source":["#### Partitions the dataframe has\n\nThe dataframes are internally stores as RDDs and partitions. To find number of partitions."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"de5335fb-efaa-4632-b12d-cb5822292ee5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["matches_df.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f2e94264-cbed-4ba5-9256-185adf7263fd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[11]: 1"]}],"execution_count":0},{"cell_type":"code","source":["# Define schema for deliveries.csv\ndeliveries_schema = StructType([\n    StructField(\"match_id\", IntegerType(), True),\n    StructField(\"inning\", IntegerType(), True),\n    StructField(\"batting_team\", StringType(), True),\n    StructField(\"bowling_team\", StringType(), True),\n    StructField(\"over\", IntegerType(), True),\n    StructField(\"ball\", IntegerType(), True),\n    StructField(\"batsman\", StringType(), True),\n    StructField(\"non_striker\", StringType(), True),\n    StructField(\"bowler\", StringType(), True),\n    StructField(\"is_super_over\", IntegerType(), True),\n    StructField(\"wide_runs\", IntegerType(), True),\n    StructField(\"bye_runs\", IntegerType(), True),\n    StructField(\"legbye_runs\", IntegerType(), True),\n    StructField(\"noball_runs\", IntegerType(), True),\n    StructField(\"penalty_runs\", IntegerType(), True),\n    StructField(\"batsman_runs\", IntegerType(), True),\n    StructField(\"extra_runs\", IntegerType(), True),\n    StructField(\"total_runs\", IntegerType(), True),\n    StructField(\"player_dismissed\", StringType(), True),\n    StructField(\"dismissal_kind\", StringType(), True),\n    StructField(\"fielder\", StringType(), True)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fb0124c8-ddf2-4813-9293-3979e3eb33b3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["deliveries_df = sqlContext.read.csv(\"dbfs:/FileStore/tables/ipl_datasets/deliveries.csv\", \n                                header = True, \n                                schema = StructType(deliveries_schema) )\ndeliveries_df.cache()\ndeliveries_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ec036e55-7c87-40cd-98ab-a8401d294a72","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- match_id: integer (nullable = true)\n |-- inning: integer (nullable = true)\n |-- batting_team: string (nullable = true)\n |-- bowling_team: string (nullable = true)\n |-- over: integer (nullable = true)\n |-- ball: integer (nullable = true)\n |-- batsman: string (nullable = true)\n |-- non_striker: string (nullable = true)\n |-- bowler: string (nullable = true)\n |-- is_super_over: integer (nullable = true)\n |-- wide_runs: integer (nullable = true)\n |-- bye_runs: integer (nullable = true)\n |-- legbye_runs: integer (nullable = true)\n |-- noball_runs: integer (nullable = true)\n |-- penalty_runs: integer (nullable = true)\n |-- batsman_runs: integer (nullable = true)\n |-- extra_runs: integer (nullable = true)\n |-- total_runs: integer (nullable = true)\n |-- player_dismissed: string (nullable = true)\n |-- dismissal_kind: string (nullable = true)\n |-- fielder: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### Memory Consumed to store the dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5e2a5fd7-7010-4229-bdea-62da41d960ed","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark._jvm.org.apache.spark.util.SizeEstimator.estimate(deliveries_df._jdf)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"201c9bdc-342e-42b6-9149-1d407cdc00e1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[12]: 149926624"]}],"execution_count":0},{"cell_type":"markdown","source":["#### Partitions the dataframe has\n\nThe dataframes are internally stores as RDDs and partitions. To find number of partitions."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d70fef56-62c8-41fa-962e-ca003e2c978b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["deliveries_df.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5bc795c6-81fe-4215-9207-7405957b047d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[13]: 4"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Group1_BDCC_Data_Description_Ingestion","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":163141021145769,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":882399708869921}},"nbformat":4,"nbformat_minor":0}
